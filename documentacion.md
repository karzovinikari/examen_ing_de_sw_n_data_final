##  Documentaci贸n del trabajo realizado

### Capa Bronze (Ingesta y Limpieza)

Para la primera etapa del pipeline (Bronze $\rightarrow$ Silver), implementamos una l贸gica modular utilizando **Python** y **Pandas** dentro de Airflow.

#### L贸gica de la Tarea en el DAG (`bronze_clean`)

* **Operador:** `PythonOperator`.
* **Funci贸n Wrapper:** Desarrollamos `run_bronze_clean(ds_nodash: str)` dentro del DAG para orquestar la llamada.
* **Transformaci贸n:** La l贸gica pesada de limpieza se desacopl贸 en el m贸dulo `include.transformations` (funci贸n `clean_daily_transactions`).
* **Manejo de Fechas:** Utilizamos la variable de template de Airflow `{{ ds_nodash }}` (ej: `20251206`) para identificar qu茅 archivo crudo procesar.

#### Flujo de Datos

1.  **Lectura:** El DAG recibe la fecha de ejecuci贸n, la convierte a objeto `datetime` y busca el archivo correspondiente en `data/raw/`.
2.  **Procesamiento:** Se invoca a la funci贸n de limpieza externa.
3.  **Persistencia:** Se guarda el resultado en formato **Parquet** en `data/clean/transactions_<YYYYMMDD>_clean.parquet`.
4.  **Comunicaci贸n (XCom):** La tarea retorna la ruta absoluta del archivo Parquet generado. Esto se almacena autom谩ticamente en **XCom** para que las tareas siguientes (Silver/Gold) puedan localizar el archivo sin necesidad de *hardcodear* rutas.

#### Snippet de Implementaci贸n

```python
# Definici贸n del PythonOperator para la capa Bronze
bronze_clean_task = PythonOperator(
    task_id='bronze_clean',
    python_callable=run_bronze_clean,
    op_kwargs={
        'ds_nodash': '{{ ds_nodash }}'  # Inyecci贸n de fecha de ejecuci贸n
    }
)
```
#### L贸gica de Limpieza (Capa Bronze)

La capa Bronze (`bronze_clean`) no solo convierte formatos, sino que aplica reglas de negocio estrictas para asegurar la calidad antes de que los datos entren al Data Warehouse.

Se implementa un script de Python con **Pandas** que realiza las siguientes transformaciones sobre el archivo Raw (`transactions_<ds>.csv`) antes de guardarlo como Parquet:

1.  **Normalizaci贸n de Montos (`amount`):**
    * En el archivo de origen (datos crudos) no hay filas donde el formato del monto tenga decimales con coma (`,`) pero igualmente se normalizan a punto (`.`) antes de la conversi贸n num茅rica, por si en el futuro los datos de entrada cambian de formato.
    * Se utiliza `pd.to_numeric(..., errors='coerce')` para manejar valores no num茅ricos.
    * **Regla de Negocio:** Decidimos que los valores vac铆os o inv谩lidos en `amount` no se descarten, sino que se imputen con **`0`** (`fillna(0)`). Al tener tan pocos datos (solo 6 transacciones en el ejemplo), creemos que descartar una fila es perder demasiada informaci贸n.

2.  **Normalizaci贸n de Texto (`status`):**
    * Se estandariza la columna `status` a **min煤sculas** para evitar inconsistencias en las agregaciones de dbt (ej: evitar tener "PLACED" y "placed" como estados distintos).

3.  **Filtrado de Registros Inv谩lidos:**
    * Se eliminan filas solo si faltan datos cr铆ticos de identidad o estado: `transaction_id`, `customer_id` o `status`.
    * *Nota:* No se eliminan filas por falta de `amount` ya que estas fueron tratadas en el paso 1 seg煤n indicamos en nuestra decisi贸n.

**Resultado:** El archivo resultante en `data/clean/` es un Parquet con tipos de datos correctos (float para montos, string para ids), listo para ser ingerido por dbt sin necesidad de *casting* complejo.

Estas transformaciones tambien manejan los casos en donde no haya archivos para un d铆a (dentro de la funci贸n clean_daily_transactions)


> **Nota:** 
> Si el archivo del d铆a no existe o viene con un nombre inesperado, la tarea de limpieza lanza un `AirflowSkipException`, lo que marca la tarea como skipped. Dado que las tareas downstream utilizan el trigger rule por defecto (`all_success`), tambi茅n quedan skipeadas. Esto evita procesamientos incompletos o inconsistentes y garantiza que el DAG solo avance cuando la ingesta diaria es v谩lida.

> Desde la propia UI de Airflow es posible re-ejecutar el DAG para la fecha del archivo faltante, permitiendo cargarlo correctamente una vez que haya sido depositado. En un flujo productivo ,y dado que utilizamos una estrategia "upsert" para mantener el historial (como explicaremos mas adelante), es importante que no se procese la data de d铆as posteriores hasta que el archivo faltante sea corregido, porque el pipeline asume una secuencia temporal completa y consistente.

> Alternativamente, el data owner podr铆a generar un archivo de reparaci贸n que incluya todas las transacciones del d铆a faltante, considerando estados actualizados o correcciones, evitando as铆 la dependencia estricta del archivo original.

### Capas Silver y Gold (Modelado con dbt)

Una vez generado el archivo Parquet limpio en la capa Bronze, utilizamos **dbt Core** con el adaptador de DuckDB para modelar los datos, estructurando el proyecto en dos capas l贸gicas para cumplir con la arquitectura Medallion.

#### 1.1. Capa Silver (Staging): `stg_transactions`

El modelo `stg_transactions.sql` funciona para estandarizar t茅cnicamente los datos provenientes del Parquet.

* **Lectura:** Utiliza la funci贸n `read_parquet` de DuckDB apuntando al archivo generado por la tarea anterior.
* **Casteo de Tipos:** Se aseguran los tipos de datos correctos para el an谩lisis:
    * `transaction_id` y `customer_id` se castean expl铆citamente a **TEXT/STRING**.
    * `amount` se castea a **DOUBLE**.
    * Los campos de fecha se convierten a tipos `DATE` o `TIMESTAMP` seg煤n corresponda.
* **Normalizaci贸n:** Se asegura que el campo `status` est茅 uniformemente en min煤sculas (refuerzo de la l贸gica aplicada en Bronze).
* **Filtrado de Calidad:** Se descartan filas que a煤n posean valores nulos en campos cr铆ticos (IDs) que no hayan podido ser resueltos en la capa anterior.

#### 1.2 Capa Silver (Historical): `hist_transactions`

El modelo `hist_transactions.sql` cumple la funci贸n de almacenar y mantener el historial completo de transacciones, a diferencia del modelo stg_transactions, que solo representa una vista del archivo Parquet correspondiente al d铆a procesado.

Sus caracter铆sticas principales son:

* **Fuente:** Consume directamente la vista stg_transactions, que provee los datos ya estandarizados para la fecha espec铆fica procesada por el pipeline.

* **Logica Upsert:** El modelo est谩 configurado como incremental con estrategia merge, utilizando transaction_id como unique_key. Esto permite:

    - Insertar nuevas transacciones que no exist铆an previamente.

    - Actualizar transacciones existentes cuyo contenido haya cambiado en d铆as posteriores.

* **Motivaci贸n del Enfoque:** Una alternativa habr铆a sido leer todos los Parquet hist贸ricos cada vez que se construye la tabla FCT (por ejemplo, todo un rango de fechas). Sin embargo, este enfoque no escala bien: obliga a releer archivos constantemente y una misma transaction_id puede aparecer en varios d铆as, lo que requiere revisar m煤ltiples Parquets para encontrar su versi贸n m谩s reciente. Esto aumenta mucho el costo de operacion y el tiempo de procesamiento. El enfoque incremental con merge evita este problema al procesar solo las novedades del d铆a y mantener la tabla hist贸rica actualizada de manera eficiente

#### 2. Capa Gold (Marts): `fct_customer_transactions`

El modelo `fct_customer_transactions.sql` consume los datos ya estandarizados y almacenados en `hist_transactions` para generar una tabla de hechos historicos agregada por cliente, lista para consumo anal铆tico.

* **Granularidad:** Una fila por `customer_id`.
* **M茅tricas Calculadas:**
    * `transaction_count`: Cantidad total de transacciones por cliente.
    * `total_amount_all`: Suma total del monto de todas las transacciones.
    * `total_amount_completed`: Suma condicional del monto donde `status = 'completed'`, para diferenciar el volumen de ventas real/confirmado.

#### Ejecuci贸n en el DAG (`silver_dbt_run`)

Esta etapa se ejecuta mediante un `BashOperator` que corre `dbt run`. Esto materializa ambas tablas (Silver y Gold) en el archivo `warehouse/medallion.duckdb` de manera secuencial, respetando las dependencias definidas por las referencias `{{ ref() }}` de dbt.

### Estrategia de Testing (Capa Gold)

Para garantizar la calidad de los datos en la capa final (Gold), implementamos una estrategia de validaci贸n en dbt que combina tests estructurales y reglas de negocio personalizadas.

1. Tests de Esquema (schema.yml)
En el modelo fct_customer_transactions, definimos restricciones estrictas sobre la clave primaria para asegurar la integridad de la agregaci贸n.

* Test unique en customer_id: Dado que esta tabla presenta m茅tricas agregadas por cliente, es fundamental garantizar que no existan filas duplicadas para un mismo customer_id.

* Test not_null: (Ya existente) Asegura que no haya identificadores de cliente nulos.

2. Test de Integridad de Negocio (Custom SQL)
Implementamos un test de datos personalizado (tests/assert_amounts_logic.sql) para validar la coherencia de los montos calculados.

Regla: El monto total de transacciones completadas (total_amount_completed) nunca puede ser mayor al monto total absoluto (total_amount_all).

Implementaci贸n: El test busca "registros fallidos", es decir, aquellos donde completed > all. Si la consulta devuelve filas, dbt alerta el error.

Snippet del Test (assert_amounts_logic.sql):

```SQL
-- Este test falla si encuentra clientes donde el monto completado supera al total
select
    customer_id,
    total_amount_completed,
    total_amount_all
from {{ ref('fct_customer_transactions') }}
where total_amount_completed > total_amount_all
```
### Ideas de Escalabilidad y Modelado

Como mejora a futuro para un entorno productivo de alto volumen, proponemos las siguientes evoluciones a la arquitectura actual:

* **Orquestaci贸n escalable:** Migrar de `airflow standalone` a una arquitectura distribuida con una base de datos m谩s robusta. Esto permitir铆a soportar un mayor volumen de DAGs concurrentes y paralelizar corridas hist贸ricas (*backfills*) sin bloquear el *scheduler*.

* **Bronze particionado:** Persistir los archivos limpios en un formato columnar optimizado como **Parquet**, particionado f铆sicamente por fecha (`/year=2025/month=12/day=06/`) o cliente. Esto, combinado con almacenamiento en la nube y un cat谩logo, habilitar铆a lecturas mucho m谩s r谩pidas y eficientes.

* **Gold consolidado:** Enriquecer la capa de presentaci贸n expandiendo las m茅tricas agregadas por cliente.
    * **Desglose por estado:** Calcular totales diferenciados para cada estado (`completed`, `pending`, `failed`) para permitir an谩lisis financiero y de conversi贸n m谩s precisos.
    * **M茅tricas de ciclo de vida:** Incluir `first_transaction_date` y `last_transaction_date` para facilitar an谩lisis.


* **Observabilidad y Alerting:** Evolucionar el manejo de Data Quality. En lugar de solo guardar un JSON local, enviar los resultados de los tests a un t贸pico de eventos o a un *bucket* versionado. Esto permitir铆a auditar tendencias de calidad a lo largo del tiempo y disparar alertas autom谩ticas (via Slack, por ejemplo) ante anomal铆as en los datos.

### Modo de trabajo y colaboraci贸n

Durante el desarrollo del examen, al estar sentados al lado, Ceci y Ari trabajaron de manera colaborativa, discutiendo y acordando cada decisi贸n de implementaci贸n. El flujo de trabajo no se bas贸 en commits intermedios ni en un uso intensivo de Git, sino en avanzar iterativamente, probando las implementaciones paso a paso en las distintas herramientas (Airflow, dbt, etc.).

Ceci fue quien realiz贸 los pushes al repositorio con todas las implementaciones desarrolladas en conjunto, mientras que Ari realiz贸 el push del archivo de documentaci贸n que fuimos elaborando a medida que avanz谩bamos en las diferentes etapas.

Ambas desarrollaron el modelo inicial. Luego, durante la semana, contamos con el aporte de Fran, quien complement贸 el trabajo incorporando la l贸gica de manejo de datos hist贸ricos en la capa Silver, junto con otros ajustes menores. Esta mejora permiti贸 construir una Golden Layer mucho m谩s rica, completa y 煤til para la toma de decisiones.